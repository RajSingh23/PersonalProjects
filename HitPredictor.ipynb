{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtNm01jlr+5gO3mBBKfb6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajSingh23/Projects/blob/main/HitPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "2B6b3GJl8Cl_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOlSFy9thDKy",
        "outputId": "025017e5-d5da-4989-fef9-b410b1681fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.70028067361668\n",
            "Logistic Regression ROC-AUC: 0.684333400974026\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.7530072173215717\n",
            "Decision Tree ROC-AUC: 0.7285953180342384\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.7784683239775461\n",
            "Random Forest ROC-AUC: 0.8280048147874852\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.8067361668003208\n",
            "Gradient Boosting ROC-AUC: 0.86132314418536\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['launch_speed', 'launch_angle', 'events'])\n",
        "\n",
        "# Select relevant columns\n",
        "data = data[['launch_speed', 'launch_angle', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Select features and target\n",
        "features = ['launch_speed', 'launch_angle']\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC for models that support probability prediction\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the moment the ball is hit, if we use gradient boosting, we can semi-reliably predict whether it will result in a hit or not from the exit velocity and launch angle. Now, we want to see if using a created statistic, \"launch_speed_angle\", allows us to get a similar or more accurate prediction. This statistic uses both exit velocity and launch angle to classify a batted ball as one of six categories ranging from \"weak\" to \"barrel\"."
      ],
      "metadata": {
        "id": "b_EBmBKLGyml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['launch_speed_angle', 'events'])\n",
        "\n",
        "# Select relevant columns including 'launch_speed_angle'\n",
        "data = data[['launch_speed_angle', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Select features and target\n",
        "features = ['launch_speed_angle']\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC for models that support probability prediction\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpHbb1qHFVjf",
        "outputId": "47ab4f59-de5b-40af-ac06-0eea3dd6cb07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.7089013632718525\n",
            "Logistic Regression ROC-AUC: 0.7446428571428572\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.7921010425020049\n",
            "Decision Tree ROC-AUC: 0.8121264389020071\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.7921010425020049\n",
            "Random Forest ROC-AUC: 0.8121264389020071\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.7921010425020049\n",
            "Gradient Boosting ROC-AUC: 0.8121264389020071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interestingly, the simpler models benefited from this attribute being used while the more successful, more complicated models performed worse with the \"launch_speed_angle\" attribute. From knowledge of decision trees and logistic regression, we know they can be more prone to overfitting, so this simpler combination of launch angle and exit velocity benefits them. Using ensemble methods it seems we can get a slightly better prediction if we use both launch angle and exit velocity. This exemplifies how ensemble methods resist overfitting."
      ],
      "metadata": {
        "id": "lxBj5P21Hkp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################################################################"
      ],
      "metadata": {
        "id": "bS8rHEVLtUir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It makes sense that we could make a strong prediction whether a hit occurs based on how the ball is hit, but can we make a strong prediction before the ball is even hit? Next, we're going to study count and see if we can use feature engineering to improve our models."
      ],
      "metadata": {
        "id": "V2N4DmUxiH0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we are going to train our models based on ball and strike data to get a benchmark of how well count allows us to predict whether a hit occurs on a batted ball."
      ],
      "metadata": {
        "id": "P8L5iiB56nob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['balls', 'strikes', 'events'])\n",
        "\n",
        "# Select relevant columns\n",
        "data = data[['balls', 'strikes', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Select features and target\n",
        "features = ['balls', \"strikes\"]\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQvM6joN61LR",
        "outputId": "c533aaf8-8dfb-4125-cbdb-ccbf7aa5f5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.673\n",
            "Logistic Regression ROC-AUC: 0.5158811474478691\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.673\n",
            "Decision Tree ROC-AUC: 0.5032539498616355\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.673\n",
            "Random Forest ROC-AUC: 0.501985995428748\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.673\n",
            "Gradient Boosting ROC-AUC: 0.5032539498616355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I want to do some feature engineering to see if we can create a feature that is beneficial for model performance. For our feature, I want to create different \"bins\" of counts and see if categorizing similar counts in these bins will improve our models' performances. There are twelve possible counts in baseball and first, we will try 3 groups of 4 counts based on batting average. Based on data from the 2023 season, the groups are as follows: [0-2, 1-2, 2-2, 3-2] as the worst, [2-0, 0-1, 1-1, 2-1] in the middle, and [0-0, 1-0, 3-0, 3-1] as the best. The code and performance is below:"
      ],
      "metadata": {
        "id": "8CrtCFG8ZS7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['balls', 'strikes', 'events'])\n",
        "\n",
        "# Select relevant columns\n",
        "data = data[['balls', 'strikes', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Define the count groups based on the 2023 season batting averages\n",
        "worst_counts = [(0, 2), (1, 2), (2, 2), (3, 2)]\n",
        "middle_counts = [(2, 0), (0, 1), (1, 1), (2, 1)]\n",
        "best_counts = [(0, 0), (1, 0), (3, 0), (3, 1)]\n",
        "\n",
        "def assign_count_group(row):\n",
        "    count = (row['balls'], row['strikes'])\n",
        "    if count in worst_counts:\n",
        "        return 'worst'\n",
        "    elif count in middle_counts:\n",
        "        return 'middle'\n",
        "    elif count in best_counts:\n",
        "        return 'best'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "# Create the 'count_group' feature\n",
        "data['count_group'] = data.apply(assign_count_group, axis=1)\n",
        "\n",
        "# Convert 'count_group' to categorical codes\n",
        "data['count_group'] = data['count_group'].astype('category').cat.codes\n",
        "\n",
        "# Select features and target\n",
        "features = ['count_group']\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC for models that support probability prediction\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')\n"
      ],
      "metadata": {
        "id": "dpBoDLBtodT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12ccf8b-f411-49a3-f66f-f46d0c59a13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.673\n",
            "Logistic Regression ROC-AUC: 0.517212263315021\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.673\n",
            "Decision Tree ROC-AUC: 0.517212263315021\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.673\n",
            "Random Forest ROC-AUC: 0.517212263315021\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.673\n",
            "Gradient Boosting ROC-AUC: 0.517212263315021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using these bins failed to improve the model performance in a meaningful way. Knowing just balls and strikes is not enough to improve performance over what an average baseball fan could predict. The bins are also guided by how humans look at the count, so it does not allow the model to perform better than humans."
      ],
      "metadata": {
        "id": "3POJ_9unqR1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################################################################################"
      ],
      "metadata": {
        "id": "GQmXgzIv15Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go back to trying to get the best prediction we can possibly get. Here we'll use a couple features to see if pre-pitch data can help us."
      ],
      "metadata": {
        "id": "yEWWkOx018DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['launch_angle', 'launch_speed', 'zone', 'pitcher', 'batter', 'events'])\n",
        "\n",
        "# Select relevant columns\n",
        "data = data[['launch_angle', 'launch_speed', 'zone', 'pitcher', 'batter', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Select features and target\n",
        "features = ['launch_angle', 'launch_speed', 'zone', 'pitcher', 'batter']\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXQh2ZpK7wjo",
        "outputId": "722efcfe-e533-44b2-898d-069be45a555e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.6792301523656776\n",
            "Logistic Regression ROC-AUC: 0.5022454250295159\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.7363672814755413\n",
            "Decision Tree ROC-AUC: 0.697580061983471\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.7999198075380914\n",
            "Random Forest ROC-AUC: 0.8589424070247934\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.8061347233360064\n",
            "Gradient Boosting ROC-AUC: 0.858036175472255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we consider the batter and pitcher and the part of the zone the pitch was in, we have marked improvement in random forest, but gradient boosting does not improve. Pre-hit data is not going to be able to consistently predict whether a hit occurs because there is too much variance in outcome. Now we will try to use additional data after contact to improve our model performance such as the location of the ball after it is put into play."
      ],
      "metadata": {
        "id": "21TXhweU84Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "data = pd.read_csv('savant_data.csv')\n",
        "\n",
        "# Remove samples with missing values\n",
        "data = data.dropna(subset=['launch_angle', 'launch_speed', 'hc_x', 'hc_y', 'events'])\n",
        "\n",
        "# Select relevant columns\n",
        "data = data[['launch_angle', 'launch_speed', 'hc_x', 'hc_y', 'events']]\n",
        "\n",
        "# Create 'is_hit' column based on hit events\n",
        "hit_events = ['single', 'double', 'triple', 'home_run']\n",
        "data['is_hit'] = data['events'].apply(lambda x: 1 if x in hit_events else 0)\n",
        "\n",
        "# Select features and target\n",
        "features = ['launch_angle', 'launch_speed', 'hc_x', 'hc_y']\n",
        "X = data[features]\n",
        "y = data['is_hit']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTraining {model_name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'{model_name} Accuracy: {accuracy}')\n",
        "\n",
        "    # Evaluate ROC-AUC\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_prob)\n",
        "        print(f'{model_name} ROC-AUC: {roc_auc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmD-Z9EgpImC",
        "outputId": "b4c08985-2750-47c5-e492-dc05757bfd67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Accuracy: 0.7394182547642929\n",
            "Logistic Regression ROC-AUC: 0.7847633770416379\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Accuracy: 0.8651955867602809\n",
            "Decision Tree ROC-AUC: 0.8483105590062111\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Accuracy: 0.9165496489468405\n",
            "Random Forest ROC-AUC: 0.9563670577409707\n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting Accuracy: 0.9009027081243731\n",
            "Gradient Boosting ROC-AUC: 0.9448371750632619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"hc_x\" and \"hc_y\" attributes show where the ball landed after it was hit, so all models improved based on that. If we can factor in additional factors such as wind or day vs. night, we can calculate trajectories to predict where a ball will land and that allows an analyst to help inform a hitting coach what kind of contact is going to give the batters the best chance at success on a particular day."
      ],
      "metadata": {
        "id": "obxMYkUPyhXn"
      }
    }
  ]
}